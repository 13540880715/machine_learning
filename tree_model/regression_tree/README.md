# 回归树模型

## 1. 回归树原理
在普通的决策树中，通过一个特征的离散值来进行分类，而回归树则通过大于、小于判断来对连续的数据进行分类。

```
graph TB
    X -- >5 --> 区域D1
    X -- <5 --> Y
    Y -- >2 -->区域2
    Y -- <2 -->区域3

```

如果将x、y看成平面坐标轴，则相当于是对二维的平面做一个划分，划分后的每一个区域，其输出结果用落在该区域的所有值的平均值作为结果。回归树采用【**贪心算法**】

**【注：回归树中可以特征值进行重复的判断】**

**回归树总体流程类似于分类树：** 分枝时穷举每一个特征可能的划分阈值，来寻找最优切分特征和最优切分点阈值，衡量的方法是【**平方误差最小化**】。分枝直到达到预设的终止条件（如叶子个数上限）就停止。

## 2. 过拟合问题处理
可以看出，由于回归树的【**贪心算法**】，是很容易过拟合的。

### 2.1 约束控制树的过度生长
- 限制树的深度：当达到设置好的最大深度时结束树的生长。
- 分类误差法：当树继续生长无法得到客观的分类误差减小，就停止生长。
- 叶子节点最小数据量限制：一个叶子节点的数据量过小，树停止生长。

### 2.2 剪枝
【***约束树生长的缺点就是提前扼杀了其他可能性***】，过早地终止了树的生长，我们也可以等待树生长完成以后再进行剪枝，即所谓的后剪枝，而后剪枝算法主要有以下几种：

- Reduced-Error Pruning（REP，错误率降低剪枝）。
- Pesimistic-Error Pruning（PEP，悲观错误剪枝）。
- Cost-Complexity Pruning（CCP，代价复杂度剪枝）。
- Error-Based Pruning（EBP，基于错误的剪枝）。

### 3. 正则化
L1正则化可以模型参数的个数，L2正则化可以减小模型参数的值，使更多的参数变成0，即使参数更稀疏。
如L2泛数为例
$$
L=L(w)+\lambda {\textstyle \sum_{1}^{n}}w_{i}^{2}\tag{公式1} 
$$
（公式1）为损失函数加上L2正则项，进行梯度下降时，权重的更新公式为：
```math
w_i=(1-2\eta \lambda )w_i-\eta \frac{\partial L(w)}{\partial w_i} \tag{公式2}
```
可以看出，就算梯度为0，权重依旧会逐步减小。
在CART树剪刀枝中，我们依旧使用正则化来优化模型。